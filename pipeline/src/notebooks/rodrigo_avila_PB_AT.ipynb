{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ef368c",
   "metadata": {},
   "source": [
    "# INFNET\n",
    "## Projeto de Bloco: Engenharia de Dados: Big Data [25E1_5] - AT\n",
    "### Aluno: Rodrigo Avila\n",
    "### Data: 16/06/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba29190",
   "metadata": {},
   "source": [
    "## Estudo Big Data\n",
    "\n",
    "Uma empresa irá realizar um investimento em Soluções de Tecnologia, visando modernização e aumento de competitividade, para tanto pretende:\n",
    "\n",
    "Criar um aplicativo para dispositivos móveis e site que será amplamente acessado e utilizado por milhares de usuários;\n",
    "Receber milhares de dados por segundo em tempo real através de sensores;\n",
    "Capturar posts dos usuários para realizar análise de sentimento e verificar as palavras em maior evidência (word count);\n",
    "Realizar predições de vendas.\n",
    "\n",
    "### 1) Explique, com suas palavras, para usuários de negócio os principais conceitos, técnicas e ferramentas relativas à Big Data que devem constar no projeto.\n",
    "\n",
    "- **Armazenamento**: O armazenamento dos dados deve ser feito em um banco de dados NoSQL, como o MongoDB ou Cassandra, que são mais adequados para lidar com grandes volumes de dados não estruturados. (Dados de sensores, posts dos usuários, etc.)\n",
    "- **Processamento**: O processamento dos dados deve ser feito em tempo real, utilizando ferramentas como Apache Kafka para ingestão de dados e Apache Spark para processamento em tempo real.\n",
    "- **Análise de Dados**: Podemos usar bibliotecas de ML como TensorFlow ou PyTorch para realizar predições de vendas e análise de sentimentos, além de bibliotecas como NLTK ou SpaCy para análise de sentimentos e contagem de palavras.\n",
    "\n",
    "### 2) Explique quais componentes e qual(is) arquitetura(s) utilizaria. Justifique sua resposta.\n",
    "Utilizaria uma arquitetura de Big Data, contendo os seguintes componentes:\n",
    "\n",
    "* **Data Lake**: Um Data Lake é um repositório centralizado que permite armazenar grandes volumes de dados em seu formato bruto.\n",
    "* **Data Warehouse**: Um Data Warehouse é um repositório centralizado que permite armazenar dados estruturados e não estruturados, otimizando consultas e análises.\n",
    "* **ETL (Extract, Transform, Load)**: O processo de ETL é responsável por extrair dados de diferentes fontes, transformá-los em um formato adequado e carregá-los em um Data Warehouse ou Data Lake.\n",
    "\n",
    "### 3) Defina os principais perfis de profissionais necessários para compor tal solução, com seus respectivos papéis e responsabilidades.\n",
    "\n",
    "Os principais perfis de profissionais necessários para compor a solução são:\n",
    "- **Engenheiro de Dados**: Que é responsável por projetar, construir e manter a infraestrutura de dados, incluindo Data Lakes e Data Warehouses. Ele também é responsável por garantir a qualidade e integridade dos dados.\n",
    "- **Cientista de Dados**: Responsável por analisar os dados e criar modelos preditivos, utilizando técnicas de Machine Learning e estatística.\n",
    "- **Analista de Dados**: Tem o papel de analisar os dados e criar relatórios e dashboards, utilizando ferramentas de visualização de dados, além de interpretar e apresentar os resultados para a equipe de negócios.\n",
    "\n",
    "### 4) Você adotaria uma solução de dados local (on-premise) ou em nuvem? Justifique. Você utilizaria MongoDB? Justifique sua resposta. \n",
    "\n",
    "Depende se a empresa já possui infra-estrutura local e uma equipe de TI capacitada para gerenciar essa infraestrutura.\n",
    "\n",
    "Na maioria dos casos atualmente, a solução em nuvem é mais viável, pois oferece escalabilidade, flexibilidade e redução de custos com hardware e manutenção.\n",
    "\n",
    "Por não ter o detalhamento da empresa, eu optaria por cloud pois é o modelo que funciona melhor para a maioria das empresas atualmente.\n",
    "\n",
    "Eu utilizaria o MongoDB, por se tratar de dados não estruturados e por ser escalável, permitindo o armazenamento de grandes volumes de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9909dc",
   "metadata": {},
   "source": [
    "# Objetivo do Data Lake\n",
    "Meu Data Lake tem como principal objetivo armazenar, extrair e disponibilizar dados de .EPUBs, arquivos de livros digitais e extrair os dados contidos neles, e também receber eventos em tempo real dos livros que estão sendo lidos pelos usuário, para servir como base para análises futuras e até mesmo para alimentar um sistema de recomendação.\n",
    "\n",
    "A demanda se originou da necessidade de complementar meu projeto de extensão do INFNET, que é um site para leitura de livros digitais estilo Netflix, onde temos um **dataset de livros digitais públicos no formato .EPUB**, que pretendemos disponibiliza-los para leitura online.\n",
    "\n",
    "Para isso, precisamos extrair os dados contidos nos arquivos .EPUBs, como capa (imagem), título, autor, gênero, sinopse, entre outros, e disponibilizar esses dados para consulta no frontend através de uma API Restful.\n",
    "\n",
    "## Um pouco sobre .EPUBs\n",
    "\n",
    "Os .EPUBs são arquivos compactados que contêm arquivos de diversos formatos como HTML, CSS e imagens, entre outros. Eles são usados para criar livros digitais e podem ser lidos em dispositivos como e-readers, tablets e smartphones.\n",
    "\n",
    "Apesar dos arquivos .EPUBs seguirem uma estrutura padrão, os dados não são obrigatórios e podem variar de livro para livro. Portanto, o processo de extração de dados pode ser complexo.\n",
    "\n",
    "![epubs](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/00_epub.png) \n",
    "\n",
    "## Demonstração de como estão os projetos que utilizarão os dados do datalake.\n",
    "\n",
    "### Frontend React JS (Preview)\n",
    "![frontend](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/01_frontend.png) \n",
    "![frontend](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/02_frontend.png) \n",
    "\n",
    "### Backend Java Spring Boot\n",
    "![backend](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/03_backend.png) \n",
    "![backend](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/04_backend.png) \n",
    "![backend](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/05_backend.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1848d81",
   "metadata": {},
   "source": [
    "### Requisitos iniciais do Data Lake\n",
    "1) Escolha de tecnologias e ferramentas (ex: Google Cloud Storage, AWS S3, Azure Blob Storage)\n",
    "\n",
    "2) Configuração do ambiente de armazenamento\n",
    "\n",
    "3) Organização dos dados (ex: diretórios, metadados)\n",
    "\n",
    "4) Implementação de um pipeline de ingestão de dados\n",
    "\n",
    "5) Mecanismos de segurança e controle de acesso  \n",
    "\n",
    "6) Monitoramento\n",
    "\n",
    "### Uso de Banco de Dados NoSQL:\n",
    "Eu pretendia utilizar o **MongoDB** como banco de dados NoSQL, mas desisti após verificar que o **Full Text Search** do MongoDB não está disponível na versão Community, dessa forma, optei por utilizar o **Elasticsearch**, que é um banco de dados NoSQL open source, que além de ter as features que necessito, também possui boa integração com o Kibana para visualização dos dados.\n",
    "\n",
    "### Escolha de Ambiente OnPremise ou Cloud:\n",
    "Optei por utilizar um modelo de solução **On-Premises**, pois nesse momento o volume de dados é pequeno, cerca de 5000 .EPUBs, e não há necessidade de envolver custos de cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27410f56",
   "metadata": {},
   "source": [
    "## Arquitetura do Data Lake\n",
    "\n",
    "A arquitetura do meu Data Lake é baseada na arquitetura **Lambda**, que é uma abordagem de processamento de dados em tempo real e em lote:\n",
    "\n",
    "![arquitetura](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/00_arq_pb_att.png)\n",
    "\n",
    "A arquitetura Lambda é composta por três camadas principais: a camada batch, speed e serving.\n",
    "\n",
    "A camada **batch** é responsável por processar grandes volumes de dados em lotes. Ela será usada para processar os arquivos .EPUBs e extrair os dados contidos neles.\n",
    "\n",
    "A camada **speed** é responsável por processar dados em tempo real. Ela será usada para receber eventos em tempo real dos livros que estão sendo lidos pelos usuários. Esses eventos podem incluir informações como inicio de leitura, fim da leitura, adicionar aos favoritos, entre outros. Esses eventos serão processados e armazenados para análises futuras.\n",
    "\n",
    "A camada **serving** é responsável por armazenar os dados processados e torná-los disponíveis para consulta.\n",
    "\n",
    "A camada **api** é adicional ao datalake, sendo responsável apenas por expor os dados processados para o frontend, tanto através das APIs do elastic para análises de dados no Kibana quanto para o Frontend React JS para a leitura dos livros digitais através da API Java Spring Boot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eadc38",
   "metadata": {},
   "source": [
    "## Ambiente Google Colab\n",
    "Criei um ambiente no Google Colab para demonstrar a configuração, dataset, transformações e análise de dados. \n",
    "\n",
    "*⚠️ Eu troquei o dataset à partir do TP4, portanto, o dataset do ambiente do Google Colab é diferente do dataset atual do projeto.*\n",
    "\n",
    "Link: [Google Colab TP3](https://colab.research.google.com/drive/1k13KP8T8bpEnixmUVOkHRUDZx1tQgVp9#scrollTo=hVjJPyvCI8Bt), compartilhamento feito para o email abaixo:\n",
    "\n",
    "![compartilhamento](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/09_comp.png)\n",
    "\n",
    "Realizei atividades bem mais complexas envolvendo o novo dataset de .EPUBs nos TP4 e TP5 onde querendo ou não foi requisito fazer do zero todos os passos do TP3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f505f",
   "metadata": {},
   "source": [
    "### Identificação da ferramenta de Orquestração\n",
    "A ferramenta de orquestração escolhida para automatizar o fluxo de dados em meu projeto foi o **Apache Spark**.\n",
    "\n",
    "Outras opções de ferramentas de orquestração que não foram escolhidas incluem:\n",
    "- **Apache Airflow**: Uma plataforma de orquestração de fluxo de trabalho que permite programar e monitorar tarefas. É amplamente utilizada para automação de pipelines de dados, especialmente em ambientes de Big Data.\n",
    "- **Luigi**: Uma ferramenta de orquestração de fluxo de trabalho desenvolvida pelo Spotify. É usada para construir pipelines de dados complexos, permitindo a execução de tarefas em sequência e o gerenciamento de dependências entre elas.\n",
    "- **Apache NiFi**: Uma ferramenta de automação de fluxo de dados que permite mover e transformar dados entre sistemas. É útil para integrar dados de diferentes fontes e destinos, além de permitir o monitoramento em tempo real dos\n",
    "\n",
    "Pelo que pesquisei, o **Apache Airflow** seria uma ferramenta mais adequada para orquestração de fluxo de dados no meu caso, pois é uma plataforma que permite programar e monitorar tarefas, coisas que teriam que ser feitas manualmente no Apache Spark.\n",
    "\n",
    "Porém, escolhi o Apache Spark em detrimento das outras devido a questões de conhecimento. Não fomos apresentados a nenhuma ferramenta de orquestração de fluxo de dados, e estive sem tempo aprender por fora da faculdade devido a outros projetos. (Principalmente o de extensão)\n",
    "\n",
    "O nível de conhecimento dos desenvolvedores com relação a uma tecnologia também é um fator importante a ser considerado na escolha de ferramentas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64900739",
   "metadata": {},
   "source": [
    "## Fluxo de Dados Camada Batch\n",
    "Aqui eu não utilizei o Data Set completo que temos do projeto de extensão (vários gigas), mas sim um subconjunto pequeno de EPUBs pessoais que eu tinha no meu computador, apenas para demonstrar o fluxo de dados.\n",
    "\n",
    "### Ambiente antes da execução do orquestrador (Spark Batch):\n",
    "\n",
    "##### 1) MinIO bucket `raw` com arquivos .EPUBs:\n",
    "![minio_raw](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/00_minio_raw.png)\n",
    "\n",
    "##### 2) MinIO bucket `processed` vazio:\n",
    "![minio_processed](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/01_minio_processed.png)\n",
    "\n",
    "##### 3) PostgreSQL sem a tabela `books_metadata`:\n",
    "![postgresql](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/02_postgres.png)\n",
    "\n",
    "##### 4) Elasticsearch sem o índice `books_content`:\n",
    "![elastic](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/03_elastic.png)\n",
    "\n",
    "##### 5) Execução do Spark Batch para processar os arquivos .EPUBs\n",
    "![spark](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/05_1_spark.png)\n",
    "![spark](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/05_2_spark.png)\n",
    "![spark](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/05_3_spark.png)\n",
    "\n",
    "\n",
    "### Ambiente após a execução do orquestrador (Spark Batch):\n",
    "\n",
    "##### 1) MinIO bucket `raw` sem arquivos .EPUBs:\n",
    "![minio_raw](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/06_minio_raw.png)\n",
    "\n",
    "##### 2) MinIO bucket `processed` com diretórios de arquivos processados:\n",
    "![minio_processed](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/06_minio_processed_1.png)\n",
    "\n",
    "##### 3) MinIO bucket `processed/cover` com imagens de capas extraídas:\n",
    "![minio_processed](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/06_minio_processed_2.png)\n",
    "\n",
    "##### 4) MinIO bucket `processed/epub` com arquivos EPUBs processados:\n",
    "![minio_processed](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/06_minio_processed_3.png)\n",
    "\n",
    "##### 5) PostgreSQL com metadados dos epubs disponíveis na tabela `books_metadata`:\n",
    "![postgres](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/07_postgres_att.png)\n",
    "\n",
    "##### 6) Elasticsearch com conteúdo dos epubs no índice `books_content`:\n",
    "![postgres](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/08_elastic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bbd577",
   "metadata": {},
   "source": [
    "## Fluxo de Dados Camada Speed - Analytics\n",
    "Recebe eventos em tempo real dos livros que estão sendo lidos pelos usuários, faz join com os metadados, e os persiste no Elasticsearch para análises futuras.\n",
    "\n",
    "##### 1) Aplicação Java enviando ```eventos``` para o Kafka\n",
    "![java events](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/02_java_events.png)\n",
    "\n",
    "##### 2) Spark ```processando``` eventos\n",
    "![spark speed](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/04_spark_speed.png)\n",
    "\n",
    "##### 3) Eventos chegando no ```elasticsearch```\n",
    "Obs: os eventos possuem apenas os dados abaixo:\n",
    "```python\n",
    "KAFKA_USERS_INTERACTION_SCHEMA = StructType([\n",
    "    StructField(\"processId\", StringType(), False),\n",
    "    StructField(\"userId\", LongType(), False),\n",
    "    StructField(\"bookId\", LongType(), False),\n",
    "    StructField(\"interactionType\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False)\n",
    "])\n",
    "```\n",
    "\n",
    "O Spark faz um join com os dados dos EPUBs que foram salvos pela camada Batch no PostgreSQL, o resultado é salvo no elasticsearch, conforme print abaixo:\n",
    "\n",
    "![elastic events](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/05_elasticsearch_events.png)\n",
    "\n",
    "\n",
    "##### 4) Kibana: Métricas dos eventos em tempo real\n",
    "![kibana](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/07_kibana_graphs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f9edf4",
   "metadata": {},
   "source": [
    "## Fluxo de Dados Camada Speed - Recomendações\n",
    "O fluxo da **Camada Speed - Recomendações** recebe eventos em tempo real do mesmo tópico do fluxo de dados da camada **Speed - Analytics**, e também faz um join com os dados do PostgreSQL, a diferença é que é um consumidor separado e que está interessado apenas em eventos de fim de leitura, ou seja, quando o usuário finalizou a leitura de um livro.\n",
    "\n",
    "Com base no livro que a pessoa leu, o sistema de recomendação irá buscar livros similares e salvar as recomendações no índice `user_recommendations`, o qual é utilizado pelo frontend para exibir as recomendações de livros para o usuário após a leitura de um livro.\n",
    "\n",
    "O algorítmo utilizado para recomendações é o **Butina Clustering** em conjunto com o **Jaccard Similarity**, que é uma técnica de agrupamento que agrupa livros com base em suas características e similaridade.\n",
    "\n",
    "Referência: https://ptabdata.blob.core.windows.net/files/2016/PGR2016-00042/v58_1061%20-%20Butina.pdf\n",
    "\n",
    "##### 1) Aplicação Java enviando ```eventos``` para o Kafka\n",
    "![java events](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/02_java_events_rec.png)\n",
    "\n",
    "##### 2) Spark ```processando``` eventos\n",
    "![spark speed](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/04_rec_spark_speed.png)\n",
    "\n",
    "##### 3) Recomendações chegando no ```elasticsearch```\n",
    "![elastic events](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/05_elasticsearch_rec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73caf86f",
   "metadata": {},
   "source": [
    "## Monitoração do Ambiente\n",
    "\n",
    "##### 1) Recursos usados por toda infra estrutura\n",
    "\n",
    "![docker stats](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/01_docker_stats.png)\n",
    "\n",
    "##### 2) Monitoração específica do Cluster Kafka \n",
    "![kakfa ui](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/03_kafka_ui.png)\n",
    "\n",
    "##### 3) Monitoração do spark\n",
    "![spark monit](/Users/rodrigo/Dev/infnet/5SEM/projeto_de_bloco/AT/images/06_spark_monit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d20de5",
   "metadata": {},
   "source": [
    "## Governança do Data Lake\n",
    "\n",
    "### 1. Organização e Estrutura:\n",
    "- **Estrutura de Diretórios**: Criar uma estrutura de diretórios clara para os dados, como `raw/`, `processed/`.\n",
    "- **Metadados**: Utilizar um banco de dados relacional (PostgreSQL) para armazenar metadados dos livros, como título, autor, data de publicação, etc. Isso facilita a busca e o gerenciamento dos dados.\n",
    "- **Indexação**: Utilizar Elasticsearch para indexar o conteúdo dos livros, permitindo buscas rápidas e eficientes em textos longos.\n",
    "- **Orquestração**: Utilizar Apache Spark para orquestrar o processamento dos dados, garantindo que os dados sejam processados de forma eficiente e escalável.\n",
    "- **Eventos em Tempo Real**: Utilizar Apache Kafka para receber eventos em tempo real dos livros que estão sendo lidos pelos usuários, permitindo análises em tempo real e atualizações dinâmicas dos dados.\n",
    "\n",
    "### 2. Controle de Acesso\n",
    "- **Políticas de Acesso**: Implementar políticas de acesso para controlar quem pode acessar, ler, escrever ou modificar os dados.\n",
    "- **Autenticação e Autorização**: Utilizar autenticação baseada em usuários e roles para garantir que apenas usuários autorizados possam acessar os dados. Por exemplo, o backend Java terá acesso de escrita no bucket `raw/` para fazer upload dos arquivos `.EPUB`, enquanto o Job Spark terá acesso de leitura e escrita para processar esses arquivos (mover entre camadas / upload).\n",
    "\n",
    "### 3. Qualidade dos Dados:\n",
    "- **Validação de Dados**: Implementar processos de validação para garantir que os dados sejam consistentes e estejam no formato correto. Isso é importante, especialmente para os arquivos `.EPUB`, que podem ter informações faltantes ou inconsistentes.\n",
    "- **Dados Faltantes**: Implementar mecanismos para lidar com dados faltantes, é muito provável que diversos arquivos `.EPUB` não tenham todos os metadados preenchidos, como título, autor, etc, nesse casos será definido valores padrões. No futuro, a ideia é implementar um processo de enriquecimento de dados para preencher esses campos faltantes com informações adicionais, como sinopse, gênero, etc. O backend Java Spring Boot também suporta operações de enriquecimento de dados (CRUD), permitindo que os dados sejam atualizados com informações adicionais conforme necessário.\n",
    "\n",
    "### 4. Auditoria e Monitoramento:\n",
    "- **Logs de Acesso**: Manter logs de acesso aos dados para rastrear quem acessou ou modificou os dados.\n",
    "- **Monitoramento de Performance**: Implementar ferramentas de monitoramento para acompanhar a performance do Data Lake.\n",
    "\n",
    "### 5. Backup e Recuperação:\n",
    "- **Backups Regulares**: Realizar backups regulares dos dados armazenados no Data Lake, incluindo os arquivos `.EPUB`, metadados e índices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0dfe1b",
   "metadata": {},
   "source": [
    "## Gestão de Acesso e Segurança\n",
    "\n",
    "### Docker Compose\n",
    "A infraestrutura de containers será configurada com variáveis de ambiente para gerenciar as credenciais de acesso aos serviços.\n",
    "\n",
    "#### Acessos na Camada Batch\n",
    "\n",
    "##### Minio (Object Storage)\n",
    "\n",
    "- Backend Java\n",
    "  - **Ação:** `s3:PutObject`  \n",
    "  - **Recurso:** `arn:aws:s3:::raw/*`  \n",
    "  - **Descrição:** Permissão exclusiva para fazer upload de arquivos `.ePUB` diretamente no bucket `raw/`. Não deve ter permissão de leitura ou escrita em outras áreas.\n",
    "\n",
    "- Job Spark (Batch)\n",
    "  - **Ações:** `s3:GetObject`, `s3:ListBucket`  \n",
    "    - **Recurso:** `arn:aws:s3:::raw/*`  \n",
    "    - **Descrição:** Permissão para ler os arquivos do bucket `raw/`.\n",
    "\n",
    "  - **Ação:** `s3:PutObject`  \n",
    "    - **Recurso:** `arn:aws:s3:::processed/*`  \n",
    "    - **Descrição:** Permissão para escrever os dados processados no bucket `processed/`.\n",
    "\n",
    "##### PostgreSQL (Metadados)\n",
    "\n",
    "- Job Spark (Batch)\n",
    "  - **Permissões:** `INSERT`, `UPDATE`  \n",
    "  - **Recursos:** Tabelas de metadados dos livros `books_metadata`\n",
    "  - **Descrição:** O Job precisa de permissão para criar e atualizar os registros de metadados após o processamento.\n",
    "\n",
    "- Backend Java\n",
    "  - **Permissão:** `SELECT`, `UPDATE`    \n",
    "  - **Recursos:** Tabelas de metadados dos livros `books_metadata`\n",
    "  - **Descrição:** O backend necessita de acesso de leitura para consultar os metadados e do update, pois ele também pode atualizar os metadados dos livros, com informações faltantes ou corrigidas.\n",
    "\n",
    "##### Elasticsearch (Conteúdo e Índices)\n",
    "\n",
    "- Job Spark (Batch)\n",
    "  - **Permissões:** `create_index`, `index`, `update`  \n",
    "  - **Recursos:** Índice de conteúdo dos livros `books_content`  \n",
    "  - **Descrição:** Permissão para criar o índice e inserir/atualizar os documentos com o conteúdo extraído dos `.ePUBs`.\n",
    "\n",
    "- Backend Java\n",
    "  - **Permissão:** `search`  \n",
    "  - **Recursos:** Índice de conteúdo dos livros (`books_content`)  \n",
    "  - **Descrição:** Permissão de leitura para executar as buscas textuais solicitadas pelos usuários.\n",
    "\n",
    "---\n",
    "\n",
    "#### Acessos na Camada Speed\n",
    "\n",
    "##### Kafka (Event Streaming)\n",
    "\n",
    "- Backend Java\n",
    "  - **Permissão:** `PRODUCER`  \n",
    "  - **Recurso:** Tópico específico de eventos (ex: `users_interactions`)  \n",
    "  - **Descrição:** O backend tem permissão apenas para enviar mensagens a este tópico.\n",
    "\n",
    "- Jobs Spark (Streaming)\n",
    "  - **Permissão:** `CONSUMER`  \n",
    "  - **Recurso:** Tópico `users_interactions`  \n",
    "  - **Descrição:** O job de streaming tem permissão apenas para consumir mensagens deste tópico.\n",
    "\n",
    "##### PostgreSQL (Metadados)\n",
    "\n",
    "- Jobs Spark (Streaming)\n",
    "  - **Permissão:** `SELECT`  \n",
    "  - **Recursos:** Tabelas de metadados dos livros `books_metadata` \n",
    "  - **Descrição:** Necessário para enriquecer os eventos em tempo real com informações dos livros.\n",
    "\n",
    "##### Elasticsearch (Eventos e Análises)\n",
    "\n",
    "- Jobs Spark (Streaming)\n",
    "  - **Permissões:** `index`, `update`  \n",
    "  - **Recursos:** Índice de eventos em tempo real (ex: `users_interactions` e `user_recommendations`)  \n",
    "  - **Descrição:** O job de streaming precisa de permissão para salvar os eventos processados e agregados.\n",
    "\n",
    "- Kibana (Data Visualization)\n",
    "  - **Permissão:** `read`  \n",
    "  - **Recursos:** Acesso aos índices `users_interactions` e `user_recommendations`  \n",
    "  - **Descrição:** Uma conta de serviço para o Kibana com acesso somente leitura para criar dashboards e visualizações. O acesso de usuários ao Kibana deve ser gerenciado por meio de seus próprios mecanismos de autenticação (RBAC).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bcb9e5",
   "metadata": {},
   "source": [
    "## Implementação da Camada de Segurança:\n",
    "\n",
    "Abaixo os comandos necessários para a implementação do plano de segurança para ambientes produtivos, que inclui a criação de usuários, roles e políticas de acesso para os diferentes componentes do Data Lake.\n",
    "\n",
    "### Minio (Object Storage)\n",
    "\n",
    "backend-java-policy.json\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::raw/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "spark-job-policy.json\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::raw/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::processed/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "```bash\n",
    "#'minio' é o alias/host do Minio configurado no mc (MinIO Client) \n",
    "\n",
    "#A diciona a política para o Backend Java\n",
    "mc admin policy add minio backend-java-policy backend-java-policy.json\n",
    "\n",
    "# Adiciona a política para o Job Spark\n",
    "mc admin policy add minio spark-job-policy spark-job-policy.json\n",
    "\n",
    "# Cria um usuário para o backend\n",
    "mc admin user add minio backend-user SENHA_DO_BACKEND\n",
    "\n",
    "# Cria um usuário para o job spark\n",
    "mc admin user add minio spark-user SENHA_DO_SPARK\n",
    "\n",
    "# Anexa a política ao usuário do backend\n",
    "mc admin policy set minio backend-java-policy user=backend-user\n",
    "\n",
    "# Anexa a política ao usuário do spark\n",
    "mc admin policy set minio spark-job-policy user=spark-user\n",
    "```\n",
    "\n",
    "### PostgreSQL (Metadados)\n",
    "\n",
    "```sql\n",
    "-- Role para o Job Spark, que poderá inserir e atualizar dados\n",
    "CREATE ROLE spark_job_role;\n",
    "\n",
    "-- Role para o Backend Java, que poderá ler e atualizar dados\n",
    "CREATE ROLE backend_java_role;\n",
    "\n",
    "-- Usuário para o Job Spark\n",
    "CREATE USER spark_user WITH LOGIN PASSWORD 'SENHA_DO_SPARK';\n",
    "\n",
    "-- Usuário para o Backend Java\n",
    "CREATE USER backend_java_user WITH LOGIN PASSWORD 'SENHA_DO_BACKEND';\n",
    "\n",
    "-- Concede permissões de INSERT e UPDATE na tabela para o role do Spark\n",
    "GRANT INSERT, UPDATE, DELETE ON TABLE books_metadata TO spark_job_role;\n",
    "\n",
    "-- Concede permissões de SELECT e UPDATE na tabela para o role do Backend\n",
    "GRANT SELECT, UPDATE, DELETE ON TABLE books_metadata TO backend_java_role;\n",
    "\n",
    "-- Atribui o role do Spark ao usuário do Spark\n",
    "GRANT spark_job_role TO spark_user;\n",
    "\n",
    "-- Atribui o role do Backend ao usuário do Backend\n",
    "GRANT backend_java_role TO backend_java_user;\n",
    "```\n",
    "\n",
    "### Elasticsearch (Conteúdo e Índices)\n",
    "```bash\n",
    "# Criação de roles no Elasticsearch para o Job Spark\n",
    "curl -X PUT \"http://localhost:9200/_security/role/spark_writer_role\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-u elastic:<SENHA_DO_USER_ELASTIC> \\\n",
    "-d'\n",
    "{\n",
    "  \"cluster\": [],\n",
    "  \"indices\": [\n",
    "    {\n",
    "      \"names\": [\n",
    "        \"books_metadata*\",\n",
    "        \"books_content*\",\n",
    "        \"users_interactions*\",\n",
    "        \"user_recommendations*\"\n",
    "      ],\n",
    "      \"privileges\": [\n",
    "        \"read\",\n",
    "        \"write\",\n",
    "        \"create_index\",\n",
    "        \"auto_configure\"\n",
    "\n",
    "      ],\n",
    "      \"allow_restricted_indices\": false\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "'\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Criação de roles no Elasticsearch para o Backend Java\n",
    "curl -X PUT \"http://localhost:9200/_security/role/backend_reader_role\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-u elastic:<SENHA_DO_USER_ELASTIC> \\\n",
    "-d'\n",
    "{\n",
    "  \"cluster\": [],\n",
    "  \"indices\": [\n",
    "    {\n",
    "      \"names\": [ \"books_content*\", \"user_recommendations*\" ],\n",
    "      \"privileges\": [ \"read\", ],\n",
    "      \"allow_restricted_indices\": false\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "'\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Criação de usuário spark_user no Elasticsearch\n",
    "curl -X PUT \"http://localhost:9200/_security/user/spark_user\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-u elastic:<SENHA_DO_USER_ELASTIC> \\\n",
    "-d'\n",
    "{\n",
    "  \"password\" : \"SENHA_DO_USER_SPARK\",\n",
    "  \"roles\" : [ \"spark_writer_role\" ]\n",
    "}\n",
    "'\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Criação de usuário backend_java_user no Elasticsearch\n",
    "curl -X PUT \"http://localhost:9200/_security/user/backend_java_user\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-u elastic:<SENHA_DO_USER_ELASTIC> \\\n",
    "-d'\n",
    "{\n",
    "  \"password\" : \"SENHA_DO_USER_BACKEND\",\n",
    "  \"roles\" : [ \"backend_reader_role\" ]\n",
    "}\n",
    "'\n",
    "```\n",
    "\n",
    "### Kibana (Data Visualization)\n",
    "```bash\n",
    "# Criação de role para o Kibana\n",
    "curl -X PUT \"http://localhost:9200/_security/role/kibana_analytics_viewer_role\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-u elastic:<SENHA_DO_USER_ELASTIC> \\\n",
    "-d'\n",
    "{\n",
    "  \"cluster\": [],\n",
    "  \"indices\": [\n",
    "    {\n",
    "      \"names\": [ \"users_interactions*\", \"books_content*\", \"user_recommendations*\" ],\n",
    "      \"privileges\": [ \"read\" ]\n",
    "    }\n",
    "  ],\n",
    "  \"applications\": [\n",
    "    {\n",
    "      \"application\": \"kibana-.kibana\",\n",
    "      \"privileges\": [ \"all\" ],\n",
    "      \"resources\": [ \"space:default\" ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "'\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Atribuindo a role ao usuário do Kibana\n",
    "curl -X PUT \"http://localhost:9200/_security/user/data_analyst\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-u elastic:<SENHA_DO_USER_ELASTIC> \\\n",
    "-d'\n",
    "{\n",
    "  \"password\" : \"SENHA_DO_USER_ANALISTA\",\n",
    "  \"roles\" : [ \"kibana_analytics_viewer_role\" ],\n",
    "  \"full_name\": \"Analista De Dados\",\n",
    "}\n",
    "'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f084140",
   "metadata": {},
   "source": [
    "## Infraestrutura\n",
    "\n",
    "Minha infraestutura e configuração do data lake foi desenvolvido totalmente utilizando Docker Compose.\n",
    "\n",
    "Por enquanto estou deixando a aplicação Java Spring com Boot, assim como o Frontend em Javascript com React em um docker compose separado, pois não fazem parte da infra estrutura do data lake.\n",
    "\n",
    "A configuração do ambiente pode ser encontrada dentro do diretório `src`, arquivos: `Dockerfile`, `docker-compose.yml` e `requirements.txt.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6827edb0d89be4",
   "metadata": {},
   "source": [
    "## Código Fonte\n",
    "Os códigos utilizados podem ser encontrados dentro do diretório `src` na raiz do projeto, sub diretórios: `spark-work` e `scripts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2146391",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook rodrigo_avila_PB_AT.ipynb to webpdf\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 5171017 bytes to rodrigo_avila_PB_AT.pdf\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to webpdf rodrigo_avila_PB_AT.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
