import os
from typing import List

import boto3
import psycopg2
from botocore.client import Config
from elasticsearch import Elasticsearch
from elasticsearch.exceptions import ApiError
from psycopg2 import sql
from pyspark.sql import DataFrame
from pyspark.sql.functions import from_json, col, date_format
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
import pickle
import os


BUTINA_RECOMENTADTIONS_CLUSTERS_PATH = os.environ.get("BUTINA_RECOMENTADTIONS_CLUSTERS_PATH", "/work/butina_clusters.pkl")

USO_UTC_8601_DATE_FORMAT = "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"

RAW_BUCKET_NAME = os.environ.get("RAW_BUCKET_NAME", "raw")
PROCESSED_BUCKET_NAME = os.environ.get("PROCESSED_BUCKET_NAME", "processed")
ELASTIC_BOOKS_CONTENT_INDEX_NAME = os.environ.get("ELASTIC_BOOKS_CONTENT_INDEX_NAME", "books_content")
ELASTIC_USERS_INTERACTION_INDEX_NAME = os.environ.get("ELASTIC_USERS_INTERACTION_INDEX_NAME", "user-interactions")
ELASTIC_USER_RECOMMENDATIONS_INDEX_NAME = os.environ.get("ELASTIC_USER_RECOMMENDATIONS_INDEX_NAME", "user-recommendations")
BOOKS_METADATA_TABLE_NAME = os.environ.get("BOOKS_METADATA_TABLE_NAME", "books_metadata")
KAFKA_USERS_INTERACTION_TOPIC_NAME = os.environ.get("KAFKA_USERS_INTERACTION_TOPIC_NAME", "user-interactions")
KAFKA_BOOTSTRAP_SERVERS = os.environ.get("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092")

S3_CONFIG = {
    "endpoint_url": os.environ.get("S3_ENDPOINT_URL", "http://minio:9000"),
    "aws_access_key_id": os.environ.get("MINIO_ROOT_USER", "admin"),
    "aws_secret_access_key": os.environ.get("MINIO_ROOT_PASSWORD", "admin123"),
    "region_name": os.environ.get("S3_REGION_NAME", "us-east-1")
}

ELASTIC_CONFIG = {
    "host": os.environ.get("ELASTICSEARCH_HOST", "elasticsearch"),
    "port": os.environ.get("ELASTICSEARCH_PORT", "9200"),
    "user": os.environ.get("ELASTICSEARCH_USERNAME", "elastic"),
    "pass": os.environ.get("ELASTICSEARCH_PASSWORD", "elastic"),
}

POSTGRES_CONFIG = {
    "host": os.environ.get("POSTGRES_HOST", "postgres"),
    "port": os.environ.get("POSTGRES_PORT", "5432"),
    "database": os.environ.get("POSTGRES_DB", "postgres"),
    "user": os.environ.get("POSTGRES_USER", "postgres"),
    "password": os.environ.get("POSTGRES_PASSWORD", "postgres")
}

ELASTIC_URL = F"http://{ELASTIC_CONFIG['host']}:{ELASTIC_CONFIG['port']}"

POSTGRES_URL = f"jdbc:postgresql://{POSTGRES_CONFIG['host']}:{POSTGRES_CONFIG['port']}/{POSTGRES_CONFIG['database']}"

POSTGRES_PROPERTIES = {
    "user": POSTGRES_CONFIG["user"],
    "password": POSTGRES_CONFIG["password"],
    "driver": "org.postgresql.Driver"
}

ELASTICSEARCH_BOOKS_CONTENT_MAPPING = {
    "properties": {
        "title": {
            "type": "text",
            "fields": {
                "keyword": {"type": "keyword"}
            }
        },
        "author": {
            "type": "text",
            "fields": {
                "keyword": {"type": "keyword"}
            }
        },
        "content": {
            "type": "text"
        }
    }
}

USERS_INTERACTION_INDEX_MAPPING = {
    "properties": {
        "processId":      { "type": "keyword" },
        "userId":         { "type": "keyword" },
        "bookId":         { "type": "keyword" },
        "interactionType":{ "type": "keyword" },
        "timestamp":      { "type": "date" },
        "id":             { "type": "long" },
        "title": {
            "type": "text",
            "fields": { "keyword": { "type": "keyword" } }
        },
        "author": {
            "type": "text",
            "fields": { "keyword": { "type": "keyword" } }
        },
        "publisher":      { "type": "keyword" },
        "category":       { "type": "keyword" },
        "synopsis":       { "type": "text" },
        "release_date":   { "type": "date" },
        "pages":          { "type": "integer" },
        "cover_image_url":{ "type": "keyword" },
        "epub_url":       { "type": "keyword" },
        "created_at":     { "type": "date" },
        "updated_at":     { "type": "date" },
        "batch_id":       { "type": "integer" }
    }
}

USER_RECOMMENDATIONS_INDEX_MAPPING = {
        "properties": {
            "book_id":  { "type": "keyword" },
            "user_id":  { "type": "keyword" },
            "recommended_book_ids": {"type": "keyword"},
            "timestamp": {"type": "date"}
        }
    }

BOOKS_METADATA_TABLE_SQL = f"""
        CREATE TABLE IF NOT EXISTS {BOOKS_METADATA_TABLE_NAME}
        (
            id              BIGINT GENERATED BY DEFAULT AS IDENTITY NOT NULL,
            title           VARCHAR(255),
            author          VARCHAR(255),
            publisher       VARCHAR(255),
            category        VARCHAR(255),
            synopsis        TEXT,
            release_date    DATE,
            pages           INTEGER,
            cover_image_url TEXT,
            epub_url        TEXT,
            created_at      TIMESTAMP WITHOUT TIME ZONE,
            updated_at      TIMESTAMP WITHOUT TIME ZONE,
            CONSTRAINT pk_books_metadata PRIMARY KEY (id),
            CONSTRAINT uc_books_metadata_title UNIQUE (title),
            CONSTRAINT fk_books_metadata_title_author UNIQUE (title, author)
        )
    """

KAFKA_USERS_INTERACTION_SCHEMA = StructType([
    StructField("processId", StringType(), False),
    StructField("userId", StringType(), False),
    StructField("bookId", StringType(), False),
    StructField("interactionType", StringType(), False),
    StructField("timestamp", TimestampType(), False)
])


def ensure_postgres_table() -> None:
    print(f"🗄️ Ensuring PostgreSQL table exists...")
    conn = psycopg2.connect(**POSTGRES_CONFIG)
    cur = conn.cursor()
    create_table_query = sql.SQL(BOOKS_METADATA_TABLE_SQL)
    cur.execute(create_table_query)
    conn.commit()
    cur.close()
    conn.close()
    print("✅ PostgreSQL table is ready.")


def write_to_postgres(df, table_name: str, mode: str) -> None:
    df.write.jdbc(
        url=POSTGRES_URL,
        table=table_name,
        mode=mode,
        properties=POSTGRES_PROPERTIES
    )


def read_from_postgres(spark, table: str):
    df = spark.read.jdbc(
        url=POSTGRES_URL,
        table=table,
        properties=POSTGRES_PROPERTIES
    )
    return df


def join_interaction_to_book_metadata(df_kafka_json: DataFrame, df_books_metadata: DataFrame) -> DataFrame:
    df_kafka_postgres = df_kafka_json \
        .join(df_books_metadata, df_kafka_json.bookId == df_books_metadata.id, "left") \
        .withColumn("created_at", date_format(col("created_at"), USO_UTC_8601_DATE_FORMAT))  \
        .withColumn("updated_at", date_format(col("created_at"), USO_UTC_8601_DATE_FORMAT)) \
        .withColumn("timestamp", date_format(col("timestamp"), USO_UTC_8601_DATE_FORMAT))
    return df_kafka_postgres


def ensure_es_index(index_name: str, mappings: dict) -> None:
    try:
        es = Elasticsearch(
            ELASTIC_URL,
            basic_auth=(ELASTIC_CONFIG["user"], ELASTIC_CONFIG["pass"])
        )
        print(f"Checking if index '{index_name}' exists...")
        exists = es.indices.exists(index=index_name)
        print(f"Index exists? {exists}")

        if exists:
            print(f"Index '{index_name}' already exists.")
        else:
            if mappings is None:
                es.indices.create(index=index_name)
            else:
                es.indices.create(index=index_name, mappings=mappings)
            print(f"✅ Created index '{index_name}' successfully.")
    except ApiError as e:
        print(f"❌ API error while creating index '{index_name}': {e}")


def write_to_elasticsearch(df, index_name: str, mode: str) -> None:
    df.write.format("org.elasticsearch.spark.sql") \
        .option("es.nodes", ELASTIC_CONFIG["host"]) \
        .option("es.port", ELASTIC_CONFIG["port"]) \
        .option("es.net.http.auth.user", ELASTIC_CONFIG["user"]) \
        .option("es.net.http.auth.pass", ELASTIC_CONFIG["pass"]) \
        .option("es.resource", index_name) \
        .mode(mode) \
        .save()


def create_s3_client() -> boto3.client:
    print("🔧 Creating S3 client...")
    client = boto3.client(
        "s3",
        endpoint_url=S3_CONFIG["endpoint_url"],
        aws_access_key_id=S3_CONFIG["aws_access_key_id"],
        aws_secret_access_key=S3_CONFIG["aws_secret_access_key"],
        config=Config(signature_version="s3v4"),
        region_name=S3_CONFIG["region_name"]
    )
    print("✅ S3 client created.")
    return client


def read_kafka_stream(spark, topic_name: str, starting_offsets: str = "latest"):
    print(f"🔗 Connecting to Kafka topic '{topic_name}'...")
    df_kafka_raw = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
        .option("subscribe", topic_name) \
        .option("startingOffsets", starting_offsets) \
        .load()
    print("✅ Connected to Kafka.")
    return df_kafka_raw


def parse_kafka_json(df_kafka_raw, schema):
    df_kafka_json = df_kafka_raw.selectExpr("CAST(value AS STRING) as json") \
        .select(from_json(col("json"), schema).alias("data")) \
        .select("data.*")
    return df_kafka_json


def ensure_s3_buckets(s3_client, bucket_names: list) -> None:
    for bucket in bucket_names:
        try:
            s3_client.head_bucket(Bucket=bucket)
            print(f"✅ Bucket '{bucket}' already exists.")
        except s3_client.exceptions.NoSuchBucket:
            s3_client.create_bucket(Bucket=bucket)
            print(f"🪣 Created bucket '{bucket}'.")
        except Exception as e:
            print(f"❌ Error checking/creating bucket '{bucket}': {e}")


def list_s3_keys(s3_client, bucket_name: str, prefix: str = "") -> List[str]:
    print(f"📋 Listing keys in bucket '{bucket_name}' with prefix '{prefix}'...")
    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
    keys = [obj["Key"] for obj in response.get("Contents", [])]
    print(f"🔎 Found {len(keys)} keys.")
    return keys


def move_object_between_buckets(
    s3_client,
    source_bucket: str,
    source_key: str,
    dest_bucket: str,
    dest_key: str
) -> None:
    try:
        print(
            f"🚚 Moving object from {source_bucket}/{source_key} to {dest_bucket}/{dest_key}..."
        )
        copy_source = {
            'Bucket': source_bucket,
            'Key': source_key
        }
        s3_client.copy(copy_source, dest_bucket, dest_key)
        s3_client.delete_object(Bucket=source_bucket, Key=source_key)
        print(f"✅ Moved {source_key} from {source_bucket} to {dest_bucket}/{dest_key}")
    except Exception as e:
        print(f"❌ Failed to move {source_key}: {e}")


def save_clusters(clusters, path):
    with open(path, "wb") as f:
        pickle.dump(clusters, f)
    print(f"✅ Clusters saved to {path}")


def load_clusters(path):
    if os.path.exists(path):
        with open(path, "rb") as f:
            clusters = pickle.load(f)
        print(f"✅ Clusters loaded from {path}")
        return clusters
    return None